{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An e-commerce company named 'Ebuss'which sells the products in various categories viz. household essentials, books, personal care products, medicines, cosmetic items, beauty products, electrical appliances, kitchen and dining products and health care products etc. \n",
    "\n",
    "Ebuss wish to compete with other competitors and even wish to expand their footprints in market, asked to build a model that will improve the recommendations given to the users given their past reviews and ratings.   \n",
    "\n",
    "Henece, it is recommended to build a sentiment-based product recommendation system, which includes the following tasks.\n",
    "\n",
    "**1) Data sourcing and sentiment analysis**\n",
    "\n",
    "**2) Building a recommendation system**\n",
    "\n",
    "**3) Improving the recommendations using the sentiment analysis model**\n",
    "\n",
    "**4) Deploying the end-to-end project with a user interface**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"font-family: Arial; font-size:1.5em;color:DeepPink;\">Task 1: Data sourcing and sentiment analysis</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Data cleaning**\n",
    "\n",
    "**2) Text preprocessing**\n",
    "\n",
    "**3) Exploratory Data Analysis**\n",
    "\n",
    "**4) Feature extraction**\n",
    "\n",
    "**5) Model Building and Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Import and Install useful packages</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\programdata\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (0.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.47.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2020.6.8)\n",
      "Requirement already satisfied: wordcloud in c:\\programdata\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.18.5)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (7.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: catboost in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: graphviz in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (0.19.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (1.18.5)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (1.0.5)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (1.5.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (3.2.2)\n",
      "Requirement already satisfied: plotly in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (5.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2020.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n",
      "Requirement already satisfied: pycrf in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn-crfsuite) (1.15.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn-crfsuite) (0.9.8)\n",
      "Requirement already satisfied: tabulate in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn-crfsuite) (0.8.9)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn-crfsuite) (4.47.0)\n",
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
      "Collecting gunicorn\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "     -------------------------------------- 79.5/79.5 kB 260.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gunicorn) (62.1.0)\n",
      "Installing collected packages: gunicorn\n",
      "Successfully installed gunicorn-20.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RGhogare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!pip install wordcloud\n",
    "!pip install catboost\n",
    "!pip install pycrf\n",
    "!pip install sklearn-crfsuite\n",
    "!pip install gensim\n",
    "!pip install gunicorn\n",
    "\n",
    "# Libraries require for data reading & data visualization \n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Libraries loadings for EDA\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "# Libraries loading for input text preprocessing\n",
    "import re, nltk, spacy, string\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# from scikit-learn and NLP libraries \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "\n",
    "# Libraries for machine to learns models\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from operator import itemgetter\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <p style=\"font-family: Arial; font-size:1.5em;color:Orange;\">Task 1.1:Input Data reading , Reading, Data cleaning and pre-processing</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and reading data\n",
    "df_sbprs = pd.read_csv(\"sample30.csv\")\n",
    "\n",
    "df_sbprs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the columns\n",
    "df_sbprs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print number of rows and columns\n",
    "df_sbprs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of rows are : 30,000**\n",
    "\n",
    "**Number of Columns are: 15**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the attribute description data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs_description = pd.read_csv(\"Data+Attribute+Description.csv\", encoding='unicode_escape')\n",
    "df_sbprs_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect missing data % in different columns\n",
    "round(100*df_sbprs.isna().sum()/len(df_sbprs),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are following three attributes(features) are needed to build the sentiment analysis viz.\n",
    "\n",
    "**1:reviews_title**\n",
    "\n",
    "**2:reviews_text**\n",
    "\n",
    "**3:user_sentiment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### also in reviews_title column there are 0.63 missing values and we can replace that with blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Reviews_title replace missing values with blank\n",
    "df_sbprs['reviews_title'].fillna(\"\", inplace = True)\n",
    "\n",
    "# Drop the missing value in other two columns viz.reviews_text and user_sentiment  if any \n",
    "df_sbprs.dropna(subset=['reviews_text'], inplace=True)\n",
    "df_sbprs.dropna(subset=['user_sentiment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect missing data % in different columns\n",
    "round(100*df_sbprs.isna().sum()/len(df_sbprs),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It shows, there is 1 missing value in user_sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insecting reviews_text and reviews_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs['reviews_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs['reviews_title'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets combine reviews_title and reviews_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining  two columns 'reviews_text' and 'reviews_title' as these two depcits the sentiment of reviewer and provide the name as reviews_combine \n",
    "df_sbprs['reviews_combine'] = df_sbprs['reviews_text'] + ' ' + df_sbprs['reviews_title']\n",
    "df_sbprs.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now keep only the dataframe with two columns viz. user_sentiment and reviews_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs_final = df_sbprs[['user_sentiment', 'reviews_combine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before modelling check if there is any imbalance in dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs_final['user_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df_sbprs_final['user_sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is typical case of imbalance dataset. Will handle this in model building**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <p style=\"font-family: Arial; font-size:1.5em;color:Orange;\">Task 1.2:Text preprocessing</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the text for topic modeling\n",
    "\n",
    "Once removed all the blank complaints, need to:\n",
    "\n",
    "* Make the text lowercase\n",
    "* Remove punctuation\n",
    "\n",
    "\n",
    "\n",
    "Once you have done these cleaning operations you need to perform the following:\n",
    "* Lemmatize the texts\n",
    "* Extract the POS tags of the lemmatized text and remove all the words which have tags other than NN[tag == \"NN\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(inputtext):\n",
    "    '''\n",
    "    function to create the \n",
    "    text for topic modelling \n",
    "    '''\n",
    "        \n",
    "    inputtext=inputtext.lower() #Make the text lowercase\n",
    "       \n",
    "    inputtext=re.sub(r'[%s]%re.escape(string.punctuation)','',inputtext) #Remove punctuation\n",
    "       \n",
    "    return inputtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying \"text_process\" on feature columns \n",
    "df_sbprs_final['reviews_combine'] = pd.DataFrame(df_sbprs_final['reviews_combine'].apply(lambda x: text_process(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Follow the similar procedure for Lemmatize the texts and extracting Pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_data(data):\n",
    "    \n",
    "    '''\n",
    "     function to lemmatize texts \n",
    "    \n",
    "    '''   \n",
    "    \n",
    "    store_lemms = [] # create empty list to store lemmas\n",
    "    \n",
    "    # Extract lemmas of given text and add to the list 'sent'\n",
    "    document_text = nlp(data)\n",
    "    for word in document_text:\n",
    "        store_lemms.append(word.lemma_)\n",
    "        \n",
    "    \n",
    "    return \" \".join(store_lemms)  # return joint list of lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs_final[\"reviews_combine\"] =  df_sbprs_final.apply(lambda x: lemma_data(x['reviews_combine']), axis=1)\n",
    "\n",
    "# Check the dataframe\n",
    "df_sbprs_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <p style=\"font-family: Arial; font-size:1.5em;color:Orange;\">Task 1.3:Exploratory data analysis (EDA)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code in this task to perform the following:\n",
    "\n",
    "*   Visualise the data according to the 'Complaint' character length'\n",
    "*   Using a word cloud find the top 40 words by frequency among all the articles after processing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of lengths complaints from POS_removed_complaint feature column \n",
    "length_doc = [len(i) for i in df_sbprs_final['reviews_combine']]\n",
    "length_doc[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the data \n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(30, 30))\n",
    "font = {'family' : 'Times New Roman',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 50}\n",
    "plt.rc('font', **font)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale = 3)\n",
    "\n",
    "sns.histplot(length_doc,bins=50)\n",
    "plt.title('Distribution of Reviews Character Length')\n",
    "plt.ylabel('No. of Reviewes')\n",
    "plt.xlabel('Review character length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### above distribution is right skewed. \n",
    "\n",
    "**Now we find the top 40 words by frequency among all the articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "                          background_color='White',\n",
    "                          stopwords=stopwords,\n",
    "                          width=1200, height=1000,\n",
    "                          max_words=40,\n",
    "                          max_font_size=40, \n",
    "                          random_state=42\n",
    "                         ).generate(str(df_sbprs_final['reviews_combine']))\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this data \n",
    "pickle.dump(df_sbprs_final, open('pickle/processed_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <p style=\"font-family: Arial; font-size:1.5em;color:Orange;\">Task 1.4:Feature Extraction</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the raw texts to a matrix of TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the TfidfVectorizer \n",
    "\n",
    "tfidf=TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a document term matrix using fit_transform\n",
    "\n",
    "The contents of a document term matrix are tuples of (complaint_id,token_id) tf-idf score:\n",
    "The tuples that are not there have a tf-idf score of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs_X_train_tfidf=tfidf.fit_transform(df_sbprs_final['reviews_combine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sbprs_X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to array the tf-udf vector\n",
    "print(df_sbprs_X_train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this data \n",
    "pickle.dump(tfidf.vocabulary_, open(\"pickle/tfidf_vocab.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response and target variable\n",
    "X = df_sbprs_X_train_tfidf\n",
    "y = df_sbprs_final['user_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train-Test split with 70% training set and 30% test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "\n",
    "print(\"X_train Shape {0}:\".format(X_train.shape))\n",
    "print(\"y_train Shape {0}:\".format(y_train.shape))\n",
    "print(\"X_test Shape {0}:\".format(X_test.shape))\n",
    "print(\"y_test Shape {0}:\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <p style=\"font-family: Arial; font-size:1.5em;color:Orange;\">Task 1.5:Model Building</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to use the F1 Score parameter for evalution matrix and weighted average due to class imbalance while evaluting different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalution_different_model(y_test, y_pred, model_name):\n",
    "    \n",
    "\n",
    "        \n",
    "    '''\n",
    "    Input to this function is a target test variable, target predicted variable from different models\n",
    "    then print classification report and it return none \n",
    "    y_test: actual labels\n",
    "    data : predicted labels\n",
    "    models name: viz Logistic regression, Decision tree, Random Forest, XGBoost, Naive Bayes \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    print(f\"CLASSIFICATION REPORT for {model_name}\\n\") # print classification report of given model\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"Positive\", \"Negative\"]))\n",
    "    \n",
    "    # plotting confusion matrix of given model\n",
    "    from matplotlib.pyplot import figure\n",
    "    figure(num=None, figsize=(30, 30))\n",
    "    font = {'family' : 'Times New Roman',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 50}\n",
    "    plt.rc('font', **font)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set(font_scale = 3)\n",
    "    plt.title(f\"CONFUSION MATRIX for{0}:\".format(model_name))\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    # a custom divergin palette\n",
    "    cmap = sns.diverging_palette(100, 7, s=75, l=40,\n",
    "                            n=5, center=\"light\", as_cmap=True)\n",
    "    sns.heatmap(conf_matrix, center=0, annot=True,fmt='.2f', square=True, cmap=cmap,xticklabels=[\"Positive\", \"Negative\"], yticklabels=[\"Positive\", \"Negative\"])\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model :1 #Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial run of the Multinomial Naive Bayes with default parameters\n",
    "model_name = 'NAIVE BAYES'\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "nb_y_pred = nb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score \n",
    "nb_f1_score = f1_score(y_test, nb_y_pred, average=\"weighted\")\n",
    "nb_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning to get best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_param = {\n",
    "    'alpha': (0.00001,0.0001,0.001, 0.01, 0.1,1),\n",
    "    'fit_prior':[True, False]\n",
    "}\n",
    "\n",
    "nb_grid = GridSearchCV(estimator=nb_clf, \n",
    "                       param_grid=nb_param,\n",
    "                       verbose=1,\n",
    "                       scoring='f1_weighted',\n",
    "                       n_jobs=-1,\n",
    "                       cv=5)\n",
    "nb_grid.fit(X_train, y_train)\n",
    "print(nb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running Naive Bayes with best parameters \n",
    "\n",
    "nb_clf_tuned = MultinomialNB(alpha=0.01, fit_prior=True)\n",
    "nb_clf_tuned.fit(X_train, y_train)\n",
    "nb_tuned_y_pred = nb_clf_tuned.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score \n",
    "nb_f1_score_tuned = f1_score(y_test, nb_tuned_y_pred, average=\"weighted\")\n",
    "nb_f1_score_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate  Naive Bayes classifier with best parameters\n",
    "evalution_different_model(y_test, nb_tuned_y_pred, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : **The F1 Score of Naive Bayes model with tuned parameters gives is ~0.86**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataframe to insert F1 Scores for all subsequent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_summary = pd.DataFrame([{'Model': 'Naive_Bayes','F1_Score': round(nb_f1_score_tuned, 2)}])\n",
    "f1_score_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model :2 # Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial run of the Logistic Regression\n",
    "model_name = 'Logistic Regression'\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_y_pred = lr_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score \n",
    "lr_f1_score = f1_score(y_test, lr_y_pred, average=\"weighted\")\n",
    "lr_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning to get best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_param = {\n",
    "    'penalty': ['l1', 'l2','elasticnet', 'none'],\n",
    "    'C': [0.001,0.01,0.1,1,10,100],\n",
    "    'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(estimator=lr_clf, \n",
    "                       param_grid=lr_param,\n",
    "                       verbose=1,\n",
    "                       scoring='f1_weighted',\n",
    "                       n_jobs=-1,\n",
    "                       cv=5)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "print(lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running Logistic Regression with best parameters \n",
    "\n",
    "lr_clf_tuned = LogisticRegression(C=10, penalty='l2',solver='newton-cg')\n",
    "lr_clf_tuned.fit(X_train, y_train)\n",
    "lr_tuned_y_pred = lr_clf_tuned.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score \n",
    "lr_f1_score_tuned = f1_score(y_test, lr_tuned_y_pred, average=\"weighted\")\n",
    "lr_f1_score_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate  Logistic Regression classifier with best parameters\n",
    "evalution_different_model(y_test, lr_tuned_y_pred, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : **The F1 Score of Logistic Regression model with tuned parameters gives is ~0.90**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_summary.loc[len(f1_score_summary.index)] = ['Logistic_Regression', round(lr_f1_score_tuned, 2)]\n",
    "f1_score_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model :3 # Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial run of the Decision Tree\n",
    "model_name = 'Decision Tree'\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train, y_train)\n",
    "dt_y_pred = dt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score \n",
    "dt_f1_score = f1_score(y_test, dt_y_pred, average=\"weighted\")\n",
    "dt_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning to get best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_param = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth' : [5, 10, 15, 20, 25, 30],\n",
    "    'min_samples_leaf':[1,5,10,15, 20, 25],\n",
    "    'max_features':['auto','log2','sqrt',None],\n",
    "}\n",
    "\n",
    "dt_grid = GridSearchCV(estimator=dt_clf, \n",
    "                       param_grid=dt_param,\n",
    "                       verbose=1,\n",
    "                       scoring='f1_weighted',\n",
    "                       n_jobs=-1,\n",
    "                       cv=5)\n",
    "dt_grid.fit(X_train, y_train)\n",
    "print(dt_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running Decision Tree with best parameters \n",
    "\n",
    "dt_clf_tuned = DecisionTreeClassifier(criterion='gini',max_depth=30,min_samples_leaf=1,max_features=None)\n",
    "dt_clf_tuned.fit(X_train, y_train)\n",
    "dt_tuned_y_pred = dt_clf_tuned.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score \n",
    "dt_f1_score_tuned = f1_score(y_test, dt_tuned_y_pred, average=\"weighted\")\n",
    "dt_f1_score_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate  Decision Tree classifier with best parameters\n",
    "evalution_different_model(y_test, dt_tuned_y_pred, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : **The F1 Score of Decision Tree model with tuned parameters gives is ~0.87**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_summary.loc[len(f1_score_summary.index)] = ['Decision_Tree', round(dt_f1_score_tuned, 2)]\n",
    "f1_score_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model :4 # Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial run of the Random Forest\n",
    "\n",
    "model_name = 'Random Forest'\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score \n",
    "rf_f1_score = f1_score(y_test, rf_y_pred, average=\"weighted\")\n",
    "rf_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning to get best result (OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning to improve Random Forest performance\n",
    "rf_param = {\n",
    "     'n_estimators': [100, 200, 300],\n",
    "     'criterion':['gini','entropy'],\n",
    "     'max_depth': [10, 30, 40],\n",
    "     'min_samples_split': [1, 5, 10],\n",
    "     'min_samples_leaf': [1, 5, 10],\n",
    "     'max_features': ['log2', 'sqrt', None]    \n",
    " }\n",
    "\n",
    "rf_grid = RandomizedSearchCV(estimator=rf_clf, \n",
    "                        param_distributions=rf_param,\n",
    "                        scoring='f1_weighted',\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                       cv=5)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print(rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomizedSearchCV**: It tries random combinations for a range of values and hence it is good at testing a wide range of values and normally it reaches a very good combination very fast. This is recommended for large datasets or  number of parameters to tune are more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:RED\">PLEASE NOTE</span> : As we are using <span style=\"color:Orange\">RandomizedSearchCV</span>  above, the best parameters might change during different runs, hence we have evaluated this model multiple times and found that although parameters vary during each execution but F1 score does not vary much (variation is not more than 1-2%). Hence we came up with two best different set parameters as follows viz. \n",
    "\n",
    "1. SET_1_PARAMETER_NOTE\n",
    "\n",
    "\n",
    "2. SET_2_PARAMETER_NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:RED\">SET_1_PARAMETER_NOTE</span>: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None, 'max_depth': 40, 'criterion': 'gini'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running Random Forest with best parameters :Set 1 \n",
    "rf_clf_tuned_set_1 = RandomForestClassifier(n_estimators=100, \n",
    "                                      min_samples_split=10, \n",
    "                                      min_samples_leaf=10, \n",
    "                                      max_features=None, \n",
    "                                      max_depth=40, \n",
    "                                      criterion='gini'\n",
    ")\n",
    "\n",
    "rf_clf_tuned_set_1.fit(X_train, y_train)\n",
    "rf_tuned_y_pred_set_1 = rf_clf_tuned_set_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score \n",
    "rf_f1_score_tuned_set_1 = f1_score(y_test, rf_tuned_y_pred_set_1, average=\"weighted\")\n",
    "rf_f1_score_tuned_set_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:RED\">SET_2_PARAMETER_NOTE</span>: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None, 'max_depth': 30, 'criterion': 'gini'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running Random Forest with best parameters \n",
    "rf_clf_tuned_set_2 = RandomForestClassifier(n_estimators=200, \n",
    "                                      min_samples_split=5, \n",
    "                                      min_samples_leaf=10, \n",
    "                                      max_features=None, \n",
    "                                      max_depth=30, \n",
    "                                      criterion='entropy'\n",
    ")\n",
    "\n",
    "rf_clf_tuned_set_2.fit(X_train, y_train)\n",
    "rf_tuned_y_pred_set_2 = rf_clf_tuned_set_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score \n",
    "rf_f1_score_tuned_set_2 = f1_score(y_test, rf_tuned_y_pred_set_2, average=\"weighted\")\n",
    "rf_f1_score_tuned_set_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:RED\">NOTE</span>: As it can be seen above, while evaluating F1 score for two different set of parameters viz. Set_1 and Set_2 using random forest model, the change in <span style=\"color:RED\">F1 Score</span> is very minuscule , hence  further evaluation of  random forest model is done with **Set_1 parameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Also it is observed that, after running random forest model with RandomizedSearchCV multiple times, the F1 score varies by 1-2%. Hence choosing Set_1 parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate  Random Forest classifier with best parameters (SET_1 )\n",
    "evalution_different_model(y_test, rf_tuned_y_pred_set_1, model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_summary.loc[len(f1_score_summary.index)] = ['Random_Forest', round(rf_f1_score_tuned_set_1, 2)]\n",
    "f1_score_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model :5 # XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial run of the XG Boost\n",
    "\n",
    "model_name = 'XGBoost'\n",
    "xg_clf = XGBClassifier(tree_method='gpu_hist', \n",
    "                        gpu_id=0, \n",
    "                        predictor=\"gpu_predictor\")\n",
    "xg_clf.fit(X_train, y_train)\n",
    "xg_y_pred = xg_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score \n",
    "xg_f1_score = f1_score(y_test, xg_y_pred, average=\"weighted\")\n",
    "xg_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning to get best result (OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_param = {\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [2, 6, 10],\n",
    "    'min_child_weight': [7, 11, 19],\n",
    "    'scale_pos_weight': [10, 12],\n",
    "    'n_estimators': [300, 500] \n",
    "}\n",
    "\n",
    "xg_grid = RandomizedSearchCV(estimator=xg_clf, \n",
    "                       param_distributions=xg_param,\n",
    "                       scoring='f1_weighted',\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1,\n",
    "                       cv=5)\n",
    "xg_grid.fit(X_train, y_train)\n",
    "print(xg_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomizedSearchCV**: It tries random combinations for a range of values and hence it is good at testing a wide range of values and normally it reaches a very good combination very fast. This is recommended for large datasets or  number of parameters to tune are more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:RED\">PLEASE NOTE</span> : As we are using <span style=\"color:Orange\">RandomizedSearchCV</span>  above, the best parameters might change during different runs, hence we have evaluated this model multiple times and found that although parameters vary during each execution but F1 score does not vary much (variation is not more than 1-2%). Hence we came up with two best different set parameters as follows viz. \n",
    "\n",
    "1. SET_1_PARAMETER_NOTE\n",
    "\n",
    "\n",
    "2. SET_2_PARAMETER_NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:RED\">SET_1_PARAMETER_NOTE</span>: {'scale_pos_weight': 12, 'n_estimators': 500, 'min_child_weight': 19, 'max_depth': 2, 'learning_rate': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running XG_Boost Tree with best parameters : Set 1\n",
    "\n",
    "xgb_clf_tuned_set_1 = XGBClassifier(scale_pos_weight=10, \n",
    "                              n_estimators=500, \n",
    "                              min_child_weight=7, \n",
    "                              max_depth=2, \n",
    "                              learning_rate=0.1, \n",
    "                              tree_method='gpu_hist', \n",
    "                              gpu_id=0, \n",
    "                              predictor=\"gpu_predictor\"\n",
    ")\n",
    "\n",
    "xgb_clf_tuned_set_1.fit(X_train, y_train)\n",
    "xgb_tuned_y_pred_set_1 = xgb_clf_tuned_set_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score : Set 1\n",
    "xgb_f1_score_tuned_set_1 = f1_score(y_test, xgb_tuned_y_pred_set_1, average=\"weighted\")\n",
    "xgb_f1_score_tuned_set_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:RED\">SET_2_PARAMETER_NOTE</span>: {'scale_pos_weight': 12, 'n_estimators': 300, 'min_child_weight': 11, 'max_depth': 2, 'learning_rate': 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running XG_Boost with best parameters : Set 2\n",
    "\n",
    "xgb_clf_tuned_set_2 = XGBClassifier(scale_pos_weight=10, \n",
    "                              n_estimators=500, \n",
    "                              min_child_weight=19, \n",
    "                              max_depth=2, \n",
    "                              learning_rate=0.1, \n",
    "                              tree_method='gpu_hist', \n",
    "                              gpu_id=0, \n",
    "                              predictor=\"gpu_predictor\"\n",
    ")\n",
    "\n",
    "xgb_clf_tuned_set_2.fit(X_train, y_train)\n",
    "xgb_tuned_y_pred_set_2 = xgb_clf_tuned_set_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score Set 2\n",
    "xgb_f1_score_tuned_set_2 = f1_score(y_test, xgb_tuned_y_pred_set_2, average=\"weighted\")\n",
    "xgb_f1_score_tuned_set_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:RED\">NOTE</span>: As it can be seen above, while evaluating F1 score for two different set of parameters viz. Set_1 and Set_2 using XG_BOOST model, the change in <span style=\"color:RED\">F1 Score</span> is very minuscule , hence  further evaluation of  random forest model is done with **Set_1 parameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Also it is observed that, after running XG Boost classifier model with RandomizedSearchCV multiple times, the F1 score varies by 1-2%. Hence choosing Set_1 parameters for further evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate  XG BOOST  classifier with best parameters (Set_1)\n",
    "evalution_different_model(y_test, xgb_tuned_y_pred_set_1, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_summary.loc[len(f1_score_summary.index)] = ['XGBOOST', round(xgb_f1_score_tuned_set_1, 2)]\n",
    "f1_score_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INFERNCES FROM SUPERVISED MODEL TUNING**: Logistic regersssion perform better comapred to all other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Logistic regression model on complete data set X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running Logistic Regression with best parameters on whole dataset  \n",
    "\n",
    "lr_clf_tuned = LogisticRegression(C=10, penalty='l2',solver='newton-cg',random_state=42)\n",
    "lr_clf_tuned.fit(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuned Logistic Regression model as pickle file\n",
    "pickle.dump(lr_clf_tuned, open(\"pickle/logreg_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"font-family: Arial; font-size:1.5em;color:DeepPink;\">Task 2: Building a recommendation system</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following task are performed in this task viz.\n",
    "\n",
    " 1. User-based recommendation system\n",
    "\n",
    " 2. Item-based recommendation system\n",
    "\n",
    " 3. Select best Recommendation System\n",
    " \n",
    " 4. Recommend top-20 products to user\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <p style=\"font-family: Arial; font-size:1.5em;color:Orange;\">Task 2.1:User Based recommendation system</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-based Collaborative filter (UBCF): \n",
    "\n",
    "-  User-Based Collaborative Filtering(UBCF) is a technique used to predict the products that a user may be inretssted to buy based on the basis of ratings given to that item by the other/peers users who have similar taste with that of the target user\n",
    "\n",
    "\n",
    "-  Steps for User-Based Collaborative Filtering:\n",
    "\n",
    "      **1. Finding the similarity of users to the target user**\n",
    "      \n",
    "      **2. Prediction of missing rating of an item**\n",
    "      \n",
    "      **3. recommend the top n products to the target user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbprs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this UBCF create a dataframe which contain only relevant columns\n",
    "df_ubcf = df_sbprs[['reviews_username', 'id', 'reviews_rating']]\n",
    "\n",
    "\n",
    "\n",
    "# Look at first few rows\n",
    "df_ubcf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect missing data % in different columns\n",
    "df_ubcf.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ubcf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here changing the column names\n",
    "\n",
    "df_ubcf.columns = ['user_name', 'product_id', 'rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train-Test split with 70% training set and 30% test set\n",
    "train_ubcf, test_ubcf = train_test_split(df_ubcf, test_size=0.30, random_state=42,shuffle=True)\n",
    "\n",
    "\n",
    "print(\"train Shape {0}:\".format(train_ubcf.shape))\n",
    "print(\"test Shape {0}:\".format(test_ubcf.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a pivot table with user names as index, products as attributes/features column with ratings as its values.\n",
    "# Also here we are using fillna=0 so as to give 0rating to prodcts which have not been rated \n",
    "df_ubcf_pivot = train_ubcf.pivot_table(\n",
    "    index='user_name',\n",
    "    columns='product_id',\n",
    "    values='rating'\n",
    ").fillna(0)\n",
    "\n",
    "df_ubcf_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of dataframe\n",
    "df_ubcf_pivot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as  a next step following startergy has been followed :\n",
    "    \n",
    "- Create a dummy set to remove the products which has already been rated by users.\n",
    "\n",
    "- A copy of this dummy train is used for prediction of ratings given by peers and to allow this  where 0 rating is given to products which has already been rated by user and 1 to non-ated products\n",
    "\n",
    "- dummy test is used  for evaluation. As this is evaluation phase opposite is true for test as comapred to train i.e. 1 rating to the products that have  been rated by user and 0 to the non-rated products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the train dataset \n",
    "dummy_train_ubcf = train_ubcf.copy()\n",
    "\n",
    "# Check the head of dataframe\n",
    "dummy_train_ubcf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the ratings distribution\n",
    "dummy_train_ubcf.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 rating is given to products which has already been rated by user and 1 to non-ated products\n",
    "dummy_train_ubcf['rating'] = dummy_train_ubcf['rating'].apply(lambda x: 0 if x>=1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna=1 so as to give rating to prodcts which have not been rated \n",
    "dummy_train_ubcf = dummy_train_ubcf.pivot_table(\n",
    "    index='user_name',\n",
    "    columns='product_id',\n",
    "    values='rating'\n",
    ").fillna(1)\n",
    "\n",
    "# Check the head\n",
    "dummy_train_ubcf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of dataframe\n",
    "dummy_train_ubcf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find similarity between the users by using adjusted cosine similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a user-product matrix (without deleting NaN values)\n",
    "df_pivot_wo_nan = train_ubcf.pivot_table(\n",
    "    index='user_name',\n",
    "    columns='product_id',\n",
    "    values='rating'\n",
    ")\n",
    "\n",
    "# View Head of DataFrame\n",
    "df_pivot_wo_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the rating of the products around 0 mean and hence subtracting average ratings of users for each indiviual product from each user's rating\n",
    "mean = np.nanmean(df_pivot_wo_nan, axis=1)\n",
    "df_pivot_wo_nan_subtracted = (df_pivot_wo_nan.T-mean).T\n",
    "df_pivot_wo_nan_subtracted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity Matrix with the help of  pairwise_distance function\n",
    "user_correlation = 1 - pairwise_distances(df_pivot_wo_nan_subtracted.fillna(0), metric='cosine')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "print(user_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of user similarity matrix\n",
    "user_correlation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for user based collaborative filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert 0 for negative correlations \n",
    "user_correlation[user_correlation<0]=0\n",
    "user_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted ratings of each user corresponding to each product in the given dataset\n",
    "user_predicted_ratings = np.dot(user_correlation, df_pivot_wo_nan_subtracted.fillna(0))\n",
    "user_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of predicted ratings matrix\n",
    "user_predicted_ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now ignoring the non-rated products by setting their ratings as 0 and hence multiply the dummy_train matrix with the user_predicted_ratings matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ratings of non-rated products 0 by multiplying dummy_train  user_predicted_ratings\n",
    "user_final_rating = np.multiply(user_predicted_ratings, dummy_train_ubcf)\n",
    "user_final_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 20 recommendations for the user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a non-zero rating\n",
    "user_final_rating.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 products (having non-zero rating)\n",
    "top_20_df = user_final_rating.loc['reggie'].sort_values(ascending=False)[0:20]\n",
    "top_20_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation user based collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation is always done for products that are already rated by user particualr user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use users from test data set which are there in train dataset\n",
    "df_common = test_ubcf[test_ubcf.user_name.isin(train_ubcf.user_name)]\n",
    "df_common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into the user-movie matrix (pivot form)\n",
    "df_common_ubcf_matrix = df_common.pivot_table(index='user_name', columns='product_id', values='rating')\n",
    "df_common_ubcf_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "df_common_ubcf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  users that are common in both train and test dataset filter out those "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_correlation matrix into dataframe.\n",
    "df_user_correlation_ubcf = pd.DataFrame(user_correlation)\n",
    "df_user_correlation_ubcf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index of user correlation df as index of df_subtracted\n",
    "df_user_correlation_ubcf['user_name'] = df_pivot_wo_nan_subtracted.index\n",
    "df_user_correlation_ubcf.set_index('user_name',inplace=True)\n",
    "df_user_correlation_ubcf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch user names in a list\n",
    "eval_list = df_common.user_name.tolist()\n",
    "\n",
    "# replacing column names \n",
    "df_user_correlation_ubcf.columns = df_pivot_wo_nan_subtracted.index.tolist()\n",
    "\n",
    "# Keep only user correlations common in both\n",
    "df_user_correlation_ubcf_1 =  df_user_correlation_ubcf[df_user_correlation_ubcf.index.isin(eval_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "df_user_correlation_ubcf_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only correlations of users that are common in both train and test datasets\n",
    "df_user_correlation_ubcf_2 = df_user_correlation_ubcf_1.T[df_user_correlation_ubcf_1.T.index.isin(eval_list)]\n",
    "\n",
    "df_user_correlation_ubcf_3 = df_user_correlation_ubcf_2.T\n",
    "\n",
    "df_user_correlation_ubcf_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "df_user_correlation_ubcf_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put negative correlations to 0\n",
    "df_user_correlation_ubcf_3[df_user_correlation_ubcf_3<0]=0\n",
    "\n",
    "\n",
    "arr_common_user_predicted_ratings = np.dot(df_user_correlation_ubcf_3, df_common_ubcf_matrix.fillna(0))\n",
    "arr_common_user_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the predicted ratings \n",
    "dummy_test = df_common.copy()\n",
    "\n",
    "dummy_test['rating'] = dummy_test['rating'].apply(lambda x: 1 if x>=1 else 0)\n",
    "\n",
    "dummy_test = dummy_test.pivot_table(index='user_name', columns='product_id', values='rating').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "dummy_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication of 'common_user_predicted_ratings' with 'dummy_test'\n",
    "arr_common_user_predicted_ratings = np.multiply(arr_common_user_predicted_ratings,dummy_test)\n",
    "\n",
    "arr_common_user_predicted_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the RMSE by normalizing the ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import *\n",
    "# Get a copy of 'arr_common_user_predicted_ratings\n",
    "X_arr_common_user_predicted_ratings  = arr_common_user_predicted_ratings.copy() \n",
    "\n",
    "# Filter  positive ratings\n",
    "X_arr_common_user_predicted_ratings = X_arr_common_user_predicted_ratings[X_arr_common_user_predicted_ratings>0]\n",
    "\n",
    "# Normalizing the ratings \n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "print(scaler.fit(X_arr_common_user_predicted_ratings))\n",
    "y_arr_common_user_predicted_ratings = (scaler.transform(X_arr_common_user_predicted_ratings))\n",
    "\n",
    "print(y_arr_common_user_predicted_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total non-NaN value\n",
    "all_non_nan = np.count_nonzero(~np.isnan(y_arr_common_user_predicted_ratings))\n",
    "all_non_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_arr_common_user_predicted_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_ubcf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE \n",
    "rmse_ubcf = (sum(sum((df_common_ubcf_matrix - y_arr_common_user_predicted_ratings)**2))/all_non_nan)**0.5\n",
    "print(rmse_ubcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Based Correlation filtering RMSE : `2.45`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <p style=\"font-family: Arial; font-size:1.5em;color:Orange;\">Task 2.2:Item Based Recommendation System</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Here we are exploring the relationship between the pair of items (say X user bought and also Y user). We Can get the missing rating from rating given to other item by user\n",
    "\n",
    "- 2) The first step is to generate a model finding similarity between all the item pairs.\n",
    "\n",
    "- 3) In the second step executing a recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a pivot table with user names as index, products as attributes/features column with ratings as its values.\n",
    "# Also here we are using fillna=0 so as to give 0rating to prodcts which have not been rated \n",
    "df_ibcf_pivot = train_ubcf.pivot_table(\n",
    "    index='user_name',\n",
    "    columns='product_id',\n",
    "    values='rating'\n",
    ").T\n",
    "\n",
    "df_ibcf_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the rating of the products \n",
    "mean = np.nanmean(df_ibcf_pivot, axis=1)\n",
    "df_ibcf_subtracted = (df_ibcf_pivot.T-mean).T\n",
    "df_ibcf_subtracted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity Matrix with the help of  pairwise_distance function\n",
    "item_correlation = 1 - pairwise_distances(df_ibcf_subtracted.fillna(0), metric='cosine')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "print(item_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 to replace negative correlations \n",
    "item_correlation[item_correlation<0]=0\n",
    "item_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for item based collaborative filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted ratings of each item corresponding to each product in the given dataset\n",
    "item_predicted_ratings = np.dot((df_ibcf_pivot.fillna(0).T),item_correlation,)\n",
    "item_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "item_predicted_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the above shape is same as that of 'dummy_train'\n",
    "dummy_train_ubcf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication of 'dummy_train' with 'item_predicted_ratings'\n",
    "item_final_rating = np.multiply(item_predicted_ratings,dummy_train_ubcf)\n",
    "item_final_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 products (having non-zero rating)\n",
    "top_20_df_ibcf = item_final_rating.loc['00sab00'].sort_values(ascending=False)[0:20]\n",
    "top_20_df_ibcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Item based correlation filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the users which are common in both test and train data set\n",
    "common_ibcf = test_ubcf[test_ubcf['product_id'].isin(train_ubcf['product_id'])]\n",
    "common_ibcf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Head \n",
    "common_ibcf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix form of item based data\n",
    "common_item_based_matrix = common_ibcf.pivot_table(index='user_name', columns='product_id', values='rating').T\n",
    "common_item_based_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "common_item_based_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix form to dataframe\n",
    "df_item_correlation = pd.DataFrame(item_correlation)\n",
    "df_item_correlation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting index \n",
    "df_item_correlation['product_id'] = df_ibcf_subtracted.index\n",
    "df_item_correlation.set_index('product_id',inplace=True)\n",
    "df_item_correlation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch user names in a list\n",
    "eval_list_ibcf = common_ibcf['product_id'].tolist()\n",
    "\n",
    "# replacing column names \n",
    "df_item_correlation.columns = df_ibcf_subtracted.index.tolist()\n",
    "\n",
    "# Keep only user correlations common in both\n",
    "df_item_correlation_ibcf_1 =  df_item_correlation[df_item_correlation.index.isin(eval_list_ibcf)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only correlations of users that are common in both train and test datasets\n",
    "df_item_correlation_ibcf_2 = df_item_correlation_ibcf_1.T[df_item_correlation_ibcf_1.T.index.isin(eval_list_ibcf)]\n",
    "\n",
    "df_item_correlation_ibcf_3 = df_item_correlation_ibcf_2.T\n",
    "\n",
    "df_item_correlation_ibcf_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item_correlation_ibcf_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put negative correlations to 0\n",
    "df_item_correlation_ibcf_3[df_item_correlation_ibcf_3<0]=0\n",
    "\n",
    "\n",
    "arr_common_item_predicted_ratings = np.dot(df_item_correlation_ibcf_3, common_item_based_matrix.fillna(0))\n",
    "arr_common_item_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the predicted ratings \n",
    "dummy_test_ibcf = common_ibcf.copy()\n",
    "\n",
    "dummy_test_ibcf['rating'] = dummy_test_ibcf['rating'].apply(lambda x: 1 if x>=1 else 0)\n",
    "\n",
    "dummy_test_ibcf = dummy_test_ibcf.pivot_table(index='user_name', columns='product_id', values='rating').T.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication of 'common_user_predicted_ratings' with 'dummy_test'\n",
    "arr_common_item_predicted_ratings = np.multiply(arr_common_item_predicted_ratings,dummy_test_ibcf)\n",
    "\n",
    "arr_common_item_predicted_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import *\n",
    "# Get a copy of 'arr_common_user_predicted_ratings\n",
    "X_arr_common_item_predicted_ratings  = arr_common_item_predicted_ratings.copy() \n",
    "\n",
    "# Filter  positive ratings\n",
    "X_arr_common_item_predicted_ratings = X_arr_common_item_predicted_ratings[X_arr_common_item_predicted_ratings>0]\n",
    "\n",
    "# Normalizing the ratings \n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "print(scaler.fit(X_arr_common_item_predicted_ratings))\n",
    "y_arr_common_item_predicted_ratings = (scaler.transform(X_arr_common_item_predicted_ratings))\n",
    "\n",
    "print(y_arr_common_item_predicted_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total non-NaN value\n",
    "all_non_nan_ibcf = np.count_nonzero(~np.isnan(y_arr_common_item_predicted_ratings))\n",
    "all_non_nan_ibcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_arr_common_item_predicted_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_item_based_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE \n",
    "rmse_ibcf = (sum(sum((common_item_based_matrix - y_arr_common_item_predicted_ratings)**2))/all_non_nan)**0.5\n",
    "print(rmse_ibcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1) `User based correlation filtering(UBCF)` gives less RMSE value compared to `Item based correlation filtering(IBCF)` \n",
    "\n",
    "###  2) Chossing `UBCF` over IBCF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final ratings with UBCF\n",
    "user_final_rating = np.multiply(user_predicted_ratings, dummy_train_ubcf)\n",
    "user_final_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(user_final_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final ratings in a pickle file\n",
    "pickle.dump(user_final_rating.astype('float32'), open('pickle/user_final_rating.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Task 3`: Improving the recommendations using the sentiment analysis model\n",
    "Please check model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 : Deployment of this end to end project with a user interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accomplished using Flask and Heroku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
